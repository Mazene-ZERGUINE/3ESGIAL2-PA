/** getting all the links in a passes element */ 
fun scrapAllLinks(htmlContent)
    start 
        /** geting all the <a> elements */
        links = htmlContent::get_all_links();
        /** fetching the links */
        href = links::fetch_all();

            for (i = 0 to href::capacity() , i++) {
                print(href{i};);
            }
            dataArray = Dict();
            dataArray::include(href, "links"); 
    end


fun filterContent(htmlContent)
    start 
        /** getting the div with class name article  */
        content = htmlContent::get(DIV ,"article");
        /** getting all the html elements that containes the word chaleur */
        data = content::filter("chaleur");
        
        display = data::fetch_all();
        /** fetching only the texts of with the word chaleur */
        allTexts = display{"texts"}; 

        for (i = 0 to allTexts::capacity() , i++) {
            print(allTexts{i};);
            1::println();

            data = allTexts{i};
            /** saving data in content.txt file */ 
            data::save(TXT , "content.txt");
        }
         
    end



fun main()
    start

        website = "https://www.ecologie.gouv.fr/vagues-chaleur-plan-national-anticiper"; 
        check = website::is_html();

        if (check) -> {

            htmlContent = website::scan();

            message = "scraping all the links in the" ?. website ?. "website"; 
            print(message) ;          
            1::println();

            filterContent(htmlContent);

        } else {
            print("this is not a valide url!");
        }

    end

main();